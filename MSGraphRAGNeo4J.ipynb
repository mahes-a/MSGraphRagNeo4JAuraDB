{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from https://microsoft.github.io/graphrag/posts/get_started/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Your Workspace Variables\n",
    "First let's make sure to setup the required environment variables. For details on these environment variables, and what environment variables are available, see the variables documentation.\n",
    "\n",
    "To initialize your workspace, let's first run the graphrag.index --init command. Since we have already configured a directory named .ragtest` in the previous step, we can run the following command:\n",
    "\n",
    "\n",
    "python -m graphrag.index --init --root ./ragtest\n",
    "This will create two files: .env and settings.yaml in the ./ragtest directory.\n",
    "\n",
    ".env contains the environment variables required to run the GraphRAG pipeline. If you inspect the file, you'll see a single environment variable defined, GRAPHRAG_API_KEY=<API_KEY>. This is the API key for the OpenAI API or Azure OpenAI endpoint. You can replace this with your own API key.\n",
    "settings.yaml contains the settings for the pipeline. You can modify this file to change the settings for the pipeline.\n",
    "OpenAI and Azure OpenAI\n",
    "To run in OpenAI mode, just make sure to update the value of GRAPHRAG_API_KEY in the .env file with your OpenAI API key.\n",
    "\n",
    "Azure OpenAI\n",
    "In addition, Azure OpenAI users should set the following variables in the settings.yaml file. To find the appropriate sections, just search for the llm: configuration, you should see two sections, one for the chat endpoint and one for the embeddings endpoint. Here is an example of how to configure the chat endpoint:\n",
    "\n",
    "\n",
    "type: azure_openai_chat # Or azure_openai_embedding for embeddings\n",
    "api_base: https://<instance>.openai.azure.com\n",
    "api_version: 2024-02-15-preview # You can customize this for other versions\n",
    "deployment_name: <azure_model_deployment_name>\n",
    "For more details about configuring GraphRAG, see the configuration documentation.\n",
    "To learn more about Initialization, refer to the Initialization documentation.\n",
    "For more details about using the CLI, refer to the CLI documentation.\n",
    "Running the Indexing pipeline\n",
    "Finally we'll run the pipeline!\n",
    "\n",
    "\n",
    "python -m graphrag.index --root ./ragtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ragtest\\input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import requests\n",
    "\n",
    "# Create the directory structure\n",
    "#os.makedirs('./ragtest/input', exist_ok=True)\n",
    "\n",
    "# URL of the book\n",
    "#url = 'https://www.gutenberg.org/cache/epub/24022/pg24022.txt'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "#response = requests.get(url)\n",
    "\n",
    "# Save the content to a file\n",
    "#with open('./ragtest/input/book.txt', 'wb') as file:\n",
    "#    file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m graphrag.index --init --root ./ragtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Indexing pipeline\n",
    "Finally we'll run the pipeline!\n",
    "\n",
    "\n",
    "python -m graphrag.index --root ./ragtest\n",
    "pipeline executing from the CLI\n",
    "\n",
    "This process will take some time to run. This depends on the size of your input data, what model you're using, and the text chunk size being used (these can be configured in your settings.yml file). Once the pipeline is complete, you should see a new folder called ./ragtest/output/<timestamp>/artifacts with a series of parquet files.\n",
    "\n",
    "Using the Query Engine\n",
    "Running the Query Engine\n",
    "Now let's ask some questions using this dataset.\n",
    "\n",
    "Here is an example using Global search to ask a high-level question:\n",
    "\n",
    "\n",
    "python -m graphrag.query \\\n",
    "--root ./ragtest \\\n",
    "--method global \\\n",
    "\"What are the top themes in this story?\"\n",
    "Here is an example using Local search to ask a more specific question about a particular character:\n",
    "\n",
    "\n",
    "python -m graphrag.query \\\n",
    "--root ./ragtest \\\n",
    "--method local \\\n",
    "\"Who is Scrooge, and what are his main relationships?\"\n",
    "Please refer to Query Engine docs for detailed information about how to leverage our Local and Global search mechanisms for extracting meaningful insights from data after the Indexer has wrapped up execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m graphrag.index --root ./ragtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m graphrag.query \\\n",
    "--root ./ragtest \\\n",
    "--method global \\\n",
    "\"What is the story about?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m graphrag.query \\\n",
    "--root ./ragtest \\\n",
    "--method global \\\n",
    "\"What are the top themes in this?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m graphrag.query \\\n",
    "--root ./ragtest \\\n",
    "--method local \\\n",
    "\"\" ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from https://github.com/tomasonjo/blogs/blob/master/msft_graphrag/ms_graphrag_import.ipynb\n",
    "\n",
    "https://neo4j.com/developer-blog/microsoft-graphrag-neo4j/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHRAG_FOLDER=\"ragtest\\output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a free trial of auradb from  https://neo4j.com/product/auradb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI=\"\"\n",
    "NEO4J_USERNAME=\"\"\n",
    "NEO4J_PASSWORD=\"\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_import(statement, df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Import a dataframe into Neo4j using a batched approach.\n",
    "    Parameters: statement is the Cypher query to execute, df is the dataframe to import, and batch_size is the number of rows to import in each batch.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    start_s = time.time()\n",
    "    for start in range(0,total, batch_size):\n",
    "        batch = df.iloc[start: min(start+batch_size,total)]\n",
    "        result = driver.execute_query(\"UNWIND $rows AS value \" + statement, \n",
    "                                      rows=batch.to_dict('records'),\n",
    "                                      database_=NEO4J_DATABASE)\n",
    "        print(result.summary.counters)\n",
    "    print(f'{total} rows in { time.time() - start_s} s.')    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes and Constraints\\n\",\n",
    "    \"Indexes in Neo4j are only used to find the starting points for graph queries, e.g. quickly finding two nodes to connect. Constraints exist to avoid duplicates, we create them mostly on id's of Entity types.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We use some Types as markers with two underscores before and after to distinguish them from the actual entity types.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The default relationship type here is `RELATED` but we could also infer a real relationship-type from the description or the types of the start and end-nodes.\\n\",\n",
    "    \"\\n\",\n",
    "    \"* `__Entity__`\\n\",\n",
    "    \"* `__Document__`\\n\",\n",
    "    \"* `__Chunk__`\\n\",\n",
    "    \"* `__Community__`\\n\",\n",
    "    \"* `__Covariate__`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create constraints, idempotent operation\n",
    "\n",
    "statements = \"\"\"\n",
    "create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique;\n",
    "create constraint document_id if not exists for (d:__Document__) require d.id is unique;\n",
    "create constraint entity_id if not exists for (c:__Community__) require c.community is unique;\n",
    "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique;\n",
    "create constraint entity_title if not exists for (e:__Entity__) require e.name is unique;\n",
    "create constraint entity_title if not exists for (e:__Covariate__) require e.title is unique;\n",
    "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique;\n",
    "\"\"\".split(\";\")\n",
    "\n",
    "for statement in statements:\n",
    "    if len((statement or \"\").strip()) > 0:\n",
    "        print(statement)\n",
    "        driver.execute_query(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Process\n",
    "Importing the Documents\n",
    "We're loading the parquet file for the documents and create nodes with their ids and add the title property. We don't need to store text_unit_ids as we can create the relationships and the text content is also contained in the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_documents.parquet', columns=[\"id\", \"title\"])\n",
    "doc_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import documents\n",
    "statement = \"\"\"\n",
    "MERGE (d:__Document__ {id:value.id})\n",
    "SET d += value {.title}\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, doc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Text Units\n",
    "We load the text units, create a node per id and set the text and number of tokens. Then we connect them to the documents that we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_text_units.parquet',\n",
    "                          columns=[\"id\",\"text\",\"n_tokens\",\"document_ids\"])\n",
    "text_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "MERGE (c:__Chunk__ {id:value.id})\n",
    "SET c += value {.text, .n_tokens}\n",
    "WITH c, value\n",
    "UNWIND value.document_ids AS document\n",
    "MATCH (d:__Document__ {id:document})\n",
    "MERGE (c)-[:PART_OF]->(d)\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Nodes\n",
    "For the nodes we store id, name, description, embedding (if available), human readable id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_entities.parquet',\n",
    "                            columns=[\"name\",\"type\",\"description\",\"human_readable_id\",\"id\",\"description_embedding\",\"text_unit_ids\"])\n",
    "entity_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_statement = \"\"\"\n",
    "MERGE (e:__Entity__ {id:value.id})\n",
    "SET e += value {.human_readable_id, .description, name:replace(value.name,'\"','')}\n",
    "WITH e, value\n",
    "CALL db.create.setNodeVectorProperty(e, \"description_embedding\", value.description_embedding)\n",
    "CALL apoc.create.addLabels(e, case when coalesce(value.type,\"\") = \"\" then [] else [apoc.text.upperCamelCase(replace(value.type,'\"',''))] end) yield node\n",
    "UNWIND value.text_unit_ids AS text_unit\n",
    "MATCH (c:__Chunk__ {id:text_unit})\n",
    "MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "\"\"\"\n",
    "\n",
    "batched_import(entity_statement, entity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Relationships\n",
    "For the relationships we find the source and target node by name, using the base __Entity__ type. After creating the RELATED relationships, we set the description as attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_relationships.parquet',\n",
    "                         columns=[\"source\",\"target\",\"id\",\"rank\",\"weight\",\"human_readable_id\",\"description\",\"text_unit_ids\"])\n",
    "rel_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_statement = \"\"\"\n",
    "    MATCH (source:__Entity__ {name:replace(value.source,'\"','')})\n",
    "    MATCH (target:__Entity__ {name:replace(value.target,'\"','')})\n",
    "    // not necessary to merge on id as there is only one relationship per pair\n",
    "    MERGE (source)-[rel:RELATED {id: value.id}]->(target)\n",
    "    SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}\n",
    "    RETURN count(*) as createdRels\n",
    "\"\"\"\n",
    "\n",
    "batched_import(rel_statement, rel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Communities\n",
    "For communities we import their id, title, level. We connect the __Community__ nodes to the start and end nodes of the relationships they refer to.\n",
    "\n",
    "Connecting them to the chunks they orignate from is optional, as the entites are already connected to the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_communities.parquet', \n",
    "                     columns=[\"id\",\"level\",\"title\",\"text_unit_ids\",\"relationship_ids\"])\n",
    "\n",
    "community_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Community Reports\n",
    "Fo the community reports we create nodes for each communitiy set the id, community, level, title, summary, rank, and rank_explanation and connect them to the entities they are about. For the findings we create the findings in context of the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_report_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_community_reports.parquet',\n",
    "                               columns=[\"id\",\"community\",\"level\",\"title\",\"summary\", \"findings\",\"rank\",\"rank_explanation\",\"full_content\"])\n",
    "community_report_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import communities\n",
    "community_statement = \"\"\"\n",
    "MERGE (c:__Community__ {community:value.community})\n",
    "SET c += value {.level, .title, .rank, .rank_explanation, .full_content, .summary}\n",
    "WITH c, value\n",
    "UNWIND range(0, size(value.findings)-1) AS finding_idx\n",
    "WITH c, value, finding_idx, value.findings[finding_idx] as finding\n",
    "MERGE (c)-[:HAS_FINDING]->(f:Finding {id:finding_idx})\n",
    "SET f += finding\n",
    "\"\"\"\n",
    "batched_import(community_statement, community_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Covariates\n",
    "Covariates are for instance claims on entities, we connect them to the chunks where they originate from.\n",
    "\n",
    "By default, covariates are not included in the output, so the file might not exists in your output if you didn't set the configuration to extract claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# cov_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_covariates.parquet')\n",
    "# cov_df.head(2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_statement = \"\"\"\n",
    "MERGE (c:__Covariate__ {id:value.id})\n",
    "SET c += apoc.map.clean(value, [\"text_unit_id\", \"document_ids\", \"n_tokens\"], [NULL, \"\"])\n",
    "WITH c, value\n",
    "MATCH (ch:__Chunk__ {id: value.text_unit_id})\n",
    "MERGE (ch)-[:HAS_COVARIATE]->(c)\n",
    "\"\"\"\n",
    "# batched_import(cov_statement, cov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tqdm neo4j seaborn tiktoken langchain-openai langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip until above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from neo4j import GraphDatabase, Result\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from llama_index.core.schema import TextNode\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Adjust pandas display settings\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")  # Set to None to display the full column width\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def db_query(cypher: str, params: Dict[str, Any] = {}) -> pd.DataFrame:\n",
    "    \"\"\"Executes a Cypher statement and returns a DataFrame\"\"\"\n",
    "    return driver.execute_query(\n",
    "        cypher, parameters_=params, result_transformer_=Result.to_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\"MATCH (n:__Chunk__) RETURN n.n_tokens as token_count, count(*) AS count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "    \"MATCH (n:__Entity__) RETURN n.name AS name, n.description AS description LIMIT 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\"MATCH ()-[n:RELATED]->() RETURN n.description AS description LIMIT 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "    \"\"\"\n",
    "  MATCH (n:__Community__) \n",
    "  RETURN n.title AS title, n.summary AS summary, n.full_content AS full_content LIMIT 1\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = db_query(\n",
    "    \"\"\"\n",
    "MATCH (d:__Chunk__)\n",
    "RETURN count {(d)-[:HAS_ENTITY]->()} AS entity_count\n",
    "\"\"\"\n",
    ")\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(entity_df[\"entity_count\"], kde=True, bins=15, color=\"skyblue\")\n",
    "plt.axvline(\n",
    "    entity_df[\"entity_count\"].mean(), color=\"red\", linestyle=\"dashed\", linewidth=1\n",
    ")\n",
    "plt.axvline(\n",
    "    entity_df[\"entity_count\"].median(), color=\"green\", linestyle=\"dashed\", linewidth=1\n",
    ")\n",
    "plt.xlabel(\"Entity Count\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of Entity Count\", fontsize=15)\n",
    "plt.legend(\n",
    "    {\n",
    "        \"Mean\": entity_df[\"entity_count\"].mean(),\n",
    "        \"Median\": entity_df[\"entity_count\"].median(),\n",
    "    }\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dist_df = db_query(\n",
    "    \"\"\"\n",
    "MATCH (e:__Entity__)\n",
    "RETURN count {(e)-[:RELATED]-()} AS node_degree\n",
    "\"\"\"\n",
    ")\n",
    "# Calculate mean and median\n",
    "mean_degree = np.mean(degree_dist_df[\"node_degree\"])\n",
    "percentiles = np.percentile(degree_dist_df[\"node_degree\"], [25, 50, 75, 90])\n",
    "# Create a histogram with a logarithmic scale\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(degree_dist_df[\"node_degree\"], bins=50, kde=False, color=\"blue\")\n",
    "# Use a logarithmic scale for the x-axis\n",
    "plt.yscale(\"log\")\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.title(\"Node Degree Distribution\")\n",
    "# Add mean, median, and percentile lines\n",
    "plt.axvline(\n",
    "    mean_degree,\n",
    "    color=\"red\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean: {mean_degree:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[0],\n",
    "    color=\"purple\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"25th Percentile: {percentiles[0]:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[1],\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"50th Percentile: {percentiles[1]:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[2],\n",
    "    color=\"yellow\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"75th Percentile: {percentiles[2]:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    percentiles[3],\n",
    "    color=\"brown\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"90th Percentile: {percentiles[3]:.2f}\",\n",
    ")\n",
    "# Add legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "    \"\"\"\n",
    "  MATCH (n:__Entity__) \n",
    "  RETURN n.name AS name, count{(n)-[:RELATED]-()} AS degree\n",
    "  ORDER BY degree DESC LIMIT 5\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_data = db_query(\n",
    "    \"\"\"\n",
    "  MATCH (n:__Community__)\n",
    "  RETURN n.level AS level, count{(n)-[:IN_COMMUNITY]-()} AS members\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "stats = (\n",
    "    community_data.groupby(\"level\")\n",
    "    .agg(\n",
    "        min_members=(\"members\", \"min\"),\n",
    "        max_members=(\"members\", \"max\"),\n",
    "        median_members=(\"members\", \"median\"),\n",
    "        avg_members=(\"members\", \"mean\"),\n",
    "        num_communities=(\"members\", \"count\"),\n",
    "        total_members=(\"members\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"level\", y=\"members\", data=community_data, palette=\"viridis\")\n",
    "plt.xlabel(\"Level\")\n",
    "plt.ylabel(\"Members\")\n",
    "\n",
    "# Add statistical annotations\n",
    "for i in range(stats.shape[0]):\n",
    "    level = stats[\"level\"][i]\n",
    "    max_val = stats[\"max_members\"][i]\n",
    "    text = (\n",
    "        f\"num: {stats['num_communities'][i]}\\n\"\n",
    "        f\"all_members: {stats['total_members'][i]}\\n\"\n",
    "        f\"min: {stats['min_members'][i]}\\n\"\n",
    "        f\"max: {stats['max_members'][i]}\\n\"\n",
    "        f\"med: {stats['median_members'][i]}\\n\"\n",
    "        f\"avg: {stats['avg_members'][i]:.2f}\"\n",
    "    )\n",
    "    plt.text(level, 85, text, horizontalalignment=\"center\", fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "    \"\"\"\n",
    "MATCH (n:`__Community__`)<-[:IN_COMMUNITY]-()<-[:HAS_ENTITY]-(c)\n",
    "WITH n, count(distinct c) AS chunkCount\n",
    "SET n.weight = chunkCount\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "topChunks = 3\n",
    "topCommunities = 3\n",
    "topOutsideRels = 10\n",
    "topInsideRels = 10\n",
    "topEntities = 10\n",
    "index_name = \"entity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"]=\"2024-02-15-preview\"\n",
    "\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # organization=\"...\",\n",
    "    model=\"\",\n",
    "    # model_version=\"0125\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    "    # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
    "    # azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\", If not provided, will read env variable AZURE_OPENAI_ENDPOINT\n",
    "    # api_key=... # Can provide an API key directly. If missing read env variable AZURE_OPENAI_API_KEY\n",
    "    # openai_api_version=..., # If not provided, will read env variable AZURE_OPENAI_API_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local retriever\n",
    "The local retriever starts by using vector search to identify relevant nodes, and then collects linked information and injects it into the LLM prompt.\n",
    "\n",
    "image\n",
    "\n",
    "While this diagram might look complex, it can be easily implemented. We start by identifying relevant entities using a vector similarity search based on text embeddings of entity descriptions. Once the relevant entities are identified, we can traverse to related text chunks, relationships, community summaries, and so on. The pattern of using vector similarity search and then traversing throughout the graph can easily be implemented using a retrieval_query feature in both LangChain and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "db_query(\n",
    "    \"\"\"\n",
    "CREATE VECTOR INDEX \"\"\"\n",
    "    + index_name\n",
    "    + \"\"\" IF NOT EXISTS FOR (e:__Entity__) ON e.description_embedding\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: 1536,\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "    \"\"\"\n",
    "MATCH (n:`__Community__`)<-[:IN_COMMUNITY]-()<-[:HAS_ENTITY]-(c)\n",
    "WITH n, count(distinct c) AS chunkCount\n",
    "SET n.weight = chunkCount\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Cypher query performs multiple analytical operations on a set of nodes to extract and organize related text data:\n",
    "\n",
    "Entity-Text Unit Mapping: For each node, the query identifies linked text chunks (__Chunk__), aggregates them by the number of distinct nodes associated with each chunk, and orders them by frequency. The top chunks are returned as text_mapping.\n",
    "Entity-Report Mapping: For each node, the query finds the associated community (__Community__), and returns the summary of the top-ranked communities based on rank and weight.\n",
    "Outside Relationships: This section extracts descriptions of relationships (RELATED) where the related entity (m) is not part of the initial node set. The relationships are ranked and limited to the top external relationships.\n",
    "Inside Relationships: Similarly to outside relationships, but this time it considers only relationships where both entities are within the initial set of nodes.\n",
    "Entities Description: Simply collects descriptions of each node in the initial set. Finally, the query combines the collected data into a structured result comprising of chunks, reports, internal and external relationships, and entity descriptions, along with a default score and an empty metadata object. You have the option to remove some of the retrieval parts to test how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lc_retrieval_query = \"\"\"\n",
    "WITH collect(node) as nodes\n",
    "// Entity - Text Unit Mapping\n",
    "WITH\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)<-[:HAS_ENTITY]->(c:__Chunk__)\n",
    "    WITH c, count(distinct n) as freq\n",
    "    RETURN c.text AS chunkText\n",
    "    ORDER BY freq DESC\n",
    "    LIMIT $topChunks\n",
    "} AS text_mapping,\n",
    "// Entity - Report Mapping\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[:IN_COMMUNITY]->(c:__Community__)\n",
    "    WITH c, c.rank as rank, c.weight AS weight\n",
    "    RETURN c.summary \n",
    "    ORDER BY rank, weight DESC\n",
    "    LIMIT $topCommunities\n",
    "} AS report_mapping,\n",
    "// Outside Relationships \n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[r:RELATED]-(m) \n",
    "    WHERE NOT m IN nodes\n",
    "    RETURN r.description AS descriptionText\n",
    "    ORDER BY r.rank, r.weight DESC \n",
    "    LIMIT $topOutsideRels\n",
    "} as outsideRels,\n",
    "// Inside Relationships \n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n)-[r:RELATED]-(m) \n",
    "    WHERE m IN nodes\n",
    "    RETURN r.description AS descriptionText\n",
    "    ORDER BY r.rank, r.weight DESC \n",
    "    LIMIT $topInsideRels\n",
    "} as insideRels,\n",
    "// Entities description\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    RETURN n.description AS descriptionText\n",
    "} as entities\n",
    "// We don't have covariates or claims here\n",
    "RETURN {Chunks: text_mapping, Reports: report_mapping, \n",
    "       Relationships: outsideRels + insideRels, \n",
    "       Entities: entities} AS text, 1.0 AS score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "lc_vector = Neo4jVector.from_existing_index(\n",
    "    \n",
    "    embeddings,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=index_name,\n",
    "    retrieval_query=lc_retrieval_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = lc_vector.similarity_search(\n",
    "    \"what was the free cash flow?\",\n",
    "    k=topEntities,\n",
    "    params={\n",
    "        \"topChunks\": topChunks,\n",
    "        \"topCommunities\": topCommunities,\n",
    "        \"topOutsideRels\": topOutsideRels,\n",
    "        \"topInsideRels\": topInsideRels,\n",
    "    },\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global retriever\n",
    "The global retriever architecture is slightly more straightforward. It seems to iterate over all the community summaries on a specified hierarchical level, producing intermediate summaries and then generating a final response based on the intermediate summaries.\n",
    "\n",
    "image\n",
    "\n",
    "We have to decide which define in advance which hierarchical level we want to iterate over, which is a not a simple decision as we have no idea which one would work better. The higher up you go the hierarchical level, the larger the communities get, but there are fewer of them. This is the only information we have without inspecting summaries manually. Other parameters allow us to ignore communities below a rank or weight threshold, which we won't use here. We'll implement the global retriever using LangChain as use the same map and reduce prompts as in the GraphRAG paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SYSTEM_PROMPT = \"\"\"\n",
    "---Role---\n",
    "\n",
    "You are a helpful assistant responding to questions about data in the tables provided.\n",
    "\n",
    "\n",
    "---Goal---\n",
    "\n",
    "Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the input data tables.\n",
    "\n",
    "You should use the data provided in the data tables below as the primary context for generating the response.\n",
    "If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n",
    "\n",
    "Each key point in the response should have the following element:\n",
    "- Description: A comprehensive description of the point.\n",
    "- Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.\n",
    "\n",
    "The response should be JSON formatted as follows:\n",
    "{{\n",
    "    \"points\": [\n",
    "        {{\"description\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}},\n",
    "        {{\"description\": \"Description of point 2 [Data: Reports (report ids)]\", \"score\": score_value}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n",
    "\n",
    "Points supported by data should list the relevant reports as references as follows:\n",
    "\"This is an example sentence supported by data references [Data: Reports (report ids)]\"\n",
    "\n",
    "**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
    "\n",
    "For example:\n",
    "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n",
    "\n",
    "where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the provided tables.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "\n",
    "---Data tables---\n",
    "\n",
    "{context_data}\n",
    "\n",
    "---Goal---\n",
    "\n",
    "Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the input data tables.\n",
    "\n",
    "You should use the data provided in the data tables below as the primary context for generating the response.\n",
    "If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n",
    "\n",
    "Each key point in the response should have the following element:\n",
    "- Description: A comprehensive description of the point.\n",
    "- Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.\n",
    "\n",
    "The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n",
    "\n",
    "Points supported by data should list the relevant reports as references as follows:\n",
    "\"This is an example sentence supported by data references [Data: Reports (report ids)]\"\n",
    "\n",
    "**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
    "\n",
    "For example:\n",
    "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n",
    "\n",
    "where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the provided tables.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "The response should be JSON formatted as follows:\n",
    "{{\n",
    "    \"points\": [\n",
    "        {{\"description\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}},\n",
    "        {{\"description\": \"Description of point 2 [Data: Reports (report ids)]\", \"score\": score_value}}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            MAP_SYSTEM_PROMPT,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_chain = map_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCE_SYSTEM_PROMPT = \"\"\"\n",
    "---Role---\n",
    "\n",
    "You are a helpful assistant responding to questions about a dataset by synthesizing perspectives from multiple analysts.\n",
    "\n",
    "\n",
    "---Goal---\n",
    "\n",
    "Generate a response of the target length and format that responds to the user's question, summarize all the reports from multiple analysts who focused on different parts of the dataset.\n",
    "\n",
    "Note that the analysts' reports provided below are ranked in the **descending order of importance**.\n",
    "\n",
    "If you don't know the answer or if the provided reports do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n",
    "\n",
    "The final response should remove all irrelevant information from the analysts' reports and merge the cleaned information into a comprehensive answer that provides explanations of all the key points and implications appropriate for the response length and format.\n",
    "\n",
    "Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n",
    "\n",
    "The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n",
    "\n",
    "The response should also preserve all the data references previously included in the analysts' reports, but do not mention the roles of multiple analysts in the analysis process.\n",
    "\n",
    "**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 34, 46, 64, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n",
    "\n",
    "where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "\n",
    "---Target response length and format---\n",
    "\n",
    "{response_type}\n",
    "\n",
    "\n",
    "---Analyst Reports---\n",
    "\n",
    "{report_data}\n",
    "\n",
    "\n",
    "---Goal---\n",
    "\n",
    "Generate a response of the target length and format that responds to the user's question, summarize all the reports from multiple analysts who focused on different parts of the dataset.\n",
    "\n",
    "Note that the analysts' reports provided below are ranked in the **descending order of importance**.\n",
    "\n",
    "If you don't know the answer or if the provided reports do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n",
    "\n",
    "The final response should remove all irrelevant information from the analysts' reports and merge the cleaned information into a comprehensive answer that provides explanations of all the key points and implications appropriate for the response length and format.\n",
    "\n",
    "The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n",
    "\n",
    "The response should also preserve all the data references previously included in the analysts' reports, but do not mention the roles of multiple analysts in the analysis process.\n",
    "\n",
    "**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 34, 46, 64, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n",
    "\n",
    "where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "\n",
    "---Target response length and format---\n",
    "\n",
    "{response_type}\n",
    "\n",
    "Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            REDUCE_SYSTEM_PROMPT,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "reduce_chain = reduce_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    refresh_schema=False,\n",
    ")\n",
    "\n",
    "response_type: str = \"multiple paragraphs\"\n",
    "\n",
    "\n",
    "def global_retriever(query: str, level: int, response_type: str = response_type) -> str:\n",
    "    community_data = graph.query(\n",
    "        \"\"\"\n",
    "    MATCH (c:__Community__)\n",
    "    WHERE c.level = $level\n",
    "    RETURN c.full_content AS output\n",
    "    \"\"\",\n",
    "        params={\"level\": level},\n",
    "    )\n",
    "    intermediate_results = []\n",
    "    for community in tqdm(community_data, desc=\"Processing communities\"):\n",
    "        intermediate_response = map_chain.invoke(\n",
    "            {\"question\": query, \"context_data\": community[\"output\"]}\n",
    "        )\n",
    "        intermediate_results.append(intermediate_response)\n",
    "    final_response = reduce_chain.invoke(\n",
    "        {\n",
    "            \"report_data\": intermediate_results,\n",
    "            \"question\": query,\n",
    "            \"response_type\": response_type,\n",
    "        }\n",
    "    )\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_retriever(\"What is the story about?\", 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
